# ROS 2 Message Schemas for VLA Pipeline
# Feature: 004-vla-pipeline
# Date: 2025-12-09
# Purpose: Define message interfaces for Voice-Language-Action components

#=====================================================
# Audio Input Messages (Voice Command → Whisper)
#=====================================================

audio_msgs/Audio:
  description: |
    Audio data captured from microphone for speech recognition.
    Used by Whisper node to transcribe voice commands.
  fields:
    header:
      type: std_msgs/Header
      description: Timestamp and frame_id for audio data
    info:
      type: audio_common_msgs/AudioInfo
      description: Audio metadata (sample rate, channels, format)
      fields:
        channels:
          type: uint32
          description: Number of audio channels (1=mono, 2=stereo)
          example: 1
        sample_rate:
          type: uint32
          description: Sampling frequency in Hz
          example: 16000
        sample_format:
          type: string
          description: Audio encoding format
          example: "S16LE"  # 16-bit PCM Little Endian
        bitrate:
          type: uint32
          description: Bitrate for compressed formats
          example: 128000
        coding_format:
          type: string
          description: Codec used (wave, mp3, ogg)
          example: "wave"
    data:
      type: uint8[]
      description: Raw audio bytes (PCM or compressed)
      constraints: |
        - Optimal duration: 1-30 seconds (Whisper context window)
        - Sample rate: 16kHz recommended for Whisper
        - Format: Mono preferred (stereo can be downmixed)

  topic_name: /audio/input
  publishers: [audio_capture_node, microphone_driver]
  subscribers: [whisper_node]
  qos:
    reliability: BEST_EFFORT  # Real-time audio stream
    durability: VOLATILE
    history_depth: 10

#=====================================================
# Transcription Messages (Whisper → LLM Planner)
#=====================================================

std_msgs/String:
  description: |
    Simplified transcription message containing only text.
    Used when confidence/metadata not needed.
  fields:
    data:
      type: string
      description: Transcribed text from voice command
      example: "Navigate to the kitchen"

  topic_name: /voice_command/transcription
  publishers: [whisper_node]
  subscribers: [llm_planner_node, command_logger]
  qos:
    reliability: RELIABLE  # Ensure transcriptions are not lost
    durability: TRANSIENT_LOCAL  # Late subscribers get last transcription
    history_depth: 5

# Alternative: Custom Transcription Message with Metadata
whisper_msgs/Transcription:
  description: |
    Rich transcription message with confidence, language, and performance metrics.
    Enables validation and quality monitoring.
  fields:
    header:
      type: std_msgs/Header
      description: Timestamp when transcription completed
    text:
      type: string
      description: Transcribed text from audio
      example: "Bring me a drink from the kitchen"
    confidence:
      type: float32
      description: Whisper's confidence score (0.0-1.0)
      example: 0.92
      constraints: |
        - Reject transcriptions with confidence < 0.7 (too uncertain)
        - Typical good transcriptions: 0.85-0.95
    language:
      type: string
      description: Detected language code (ISO 639-1)
      example: "en"
    model_variant:
      type: string
      description: Whisper model used (tiny, base, small, medium, large)
      example: "base"
      enum: [tiny, base, small, medium, large]
    processing_time_ms:
      type: int32
      description: Inference latency in milliseconds
      example: 350
      constraints: |
        - Whisper base: 200-500ms typical
        - Whisper small: 500-1000ms typical
        - Target: <500ms for real-time interaction

  topic_name: /voice_command/transcription_detailed
  publishers: [whisper_node]
  subscribers: [llm_planner_node, quality_monitor_node]
  qos:
    reliability: RELIABLE
    durability: TRANSIENT_LOCAL
    history_depth: 5

#=====================================================
# Cognitive Plan Messages (LLM → Action Orchestrator)
#=====================================================

cognitive_plan_msgs/CognitivePlan:
  description: |
    Structured plan generated by LLM containing sequence of robot actions.
    Each step maps to a specific robot capability (navigate, detect, grasp, etc.)
  fields:
    header:
      type: std_msgs/Header
      description: Timestamp when plan was generated
    task_description:
      type: string
      description: Original natural language command from user
      example: "Bring me a drink from the kitchen"
    steps:
      type: cognitive_plan_msgs/PlanStep[]
      description: Ordered sequence of actions to execute
    llm_model:
      type: string
      description: LLM model used for planning (gpt-4, claude, llama-13b)
      example: "gpt-4"
    processing_time_ms:
      type: int32
      description: LLM inference latency in milliseconds
      example: 2500
      constraints: |
        - GPT-4 API: 2000-5000ms typical
        - Local Llama 13B: 500-2000ms typical
        - Target: <3000ms for responsive interaction
    confidence:
      type: float32
      description: LLM's self-assessed confidence (if available)
      example: 0.85

cognitive_plan_msgs/PlanStep:
  description: |
    Single step in cognitive plan, mapping to one robot action.
  fields:
    step_id:
      type: int32
      description: Sequential step number (1-indexed)
      example: 1
    action:
      type: string
      description: Action type (must match robot's allowed actions)
      example: "navigate_to_pose"
      enum: [navigate_to_pose, detect_objects, grasp_object, place_object, handover_object, open_door, close_gripper, wait]
    params_json:
      type: string
      description: JSON-encoded action parameters
      example: '{"location": "kitchen", "x": 5.0, "y": 3.0, "theta": 0.0}'
      constraints: |
        - Must be valid JSON
        - Parameters must match action type requirements
        - Validated before execution
    dependencies:
      type: int32[]
      description: IDs of steps that must complete before this step
      example: [1, 2]  # Step 3 depends on steps 1 and 2
      constraints: |
        - Must form directed acyclic graph (DAG) - no cycles
        - Referenced step IDs must exist in plan
    expected_duration_s:
      type: int32
      description: Estimated execution time in seconds
      example: 30
      constraints: |
        - Used for timeout detection (abort if exceeds 2x expected duration)
        - Navigation: 10-60s typical
        - Manipulation: 5-15s typical
        - Detection: 1-5s typical

  topic_name: /cognitive_plan/output
  publishers: [llm_planner_node]
  subscribers: [action_orchestrator_node, plan_visualizer]
  qos:
    reliability: RELIABLE  # Plans must not be lost
    durability: TRANSIENT_LOCAL  # Retain last plan for late subscribers
    history_depth: 3

#=====================================================
# Action Execution Messages (Action Server Goals)
#=====================================================

# NavigateToPose (Nav2 Action)
nav2_msgs/action/NavigateToPose:
  description: |
    Navigate robot to a specific pose in the map frame.
    Primary action for mobile robot locomotion.
  goal:
    pose:
      type: geometry_msgs/PoseStamped
      description: Target pose (position + orientation)
      fields:
        header:
          type: std_msgs/Header
          description: Frame ID (typically "map") and timestamp
        pose:
          type: geometry_msgs/Pose
          description: Position (x, y, z) and orientation (quaternion)
          example: |
            position: {x: 5.0, y: 3.0, z: 0.0}
            orientation: {x: 0.0, y: 0.0, z: 0.0, w: 1.0}  # Facing forward
    behavior_tree:
      type: string
      description: Optional custom behavior tree XML
      default: ""
  result:
    result:
      type: std_msgs/Empty
      description: Empty result (success indicated by action status)
  feedback:
    current_pose:
      type: geometry_msgs/PoseStamped
      description: Robot's current pose during navigation
    navigation_time:
      type: builtin_interfaces/Duration
      description: Elapsed time since navigation started
    estimated_time_remaining:
      type: builtin_interfaces/Duration
      description: Estimated time to goal
    number_of_recoveries:
      type: int16
      description: Number of recovery behaviors triggered
    distance_remaining:
      type: float32
      description: Euclidean distance to goal in meters
      example: 2.5

  action_server: /navigate_to_pose
  package: nav2_msgs
  typical_duration: 10-60 seconds

# DetectObjects (Custom Action for Vision)
vision_msgs/action/DetectObjects:
  description: |
    Detect objects in camera view using vision system (YOLO, Isaac ROS).
    Returns list of detected objects with bounding boxes and 3D positions.
  goal:
    classes:
      type: string[]
      description: Object classes to detect (e.g., ["cup", "bottle", "person"])
      example: [cup, bottle, glass]
    min_confidence:
      type: float32
      description: Minimum detection confidence threshold (0.0-1.0)
      example: 0.8
      constraints: |
        - Typical range: 0.6-0.9
        - Higher threshold: fewer false positives, may miss objects
        - Lower threshold: more detections, higher false positive rate
    timeout_s:
      type: int32
      description: Maximum time to wait for detections
      example: 5
  result:
    objects:
      type: vision_msgs/DetectedObject[]
      description: List of detected objects
      fields:
        id:
          type: string
          description: Unique identifier for this detection
          example: "obj_0"
        class_name:
          type: string
          description: Object class (cup, bottle, etc.)
          example: "cup"
        confidence:
          type: float32
          description: Detection confidence (0.0-1.0)
          example: 0.92
        bounding_box:
          type: vision_msgs/BoundingBox2D
          description: 2D image bounding box
          example: {center: {x: 320, y: 240}, size_x: 80, size_y: 120}
        position_3d:
          type: geometry_msgs/Point
          description: 3D position in camera frame (meters)
          example: {x: 1.2, y: 0.5, z: 0.8}
  feedback:
    detections_count:
      type: int32
      description: Number of objects detected so far
    processing_time_ms:
      type: int32
      description: Inference time per frame

  action_server: /detect_objects
  package: vision_msgs (custom)
  typical_duration: 1-5 seconds

# GraspObject (MoveIt2/Custom Action)
manipulation_msgs/action/GraspObject:
  description: |
    Grasp a detected object using manipulation arm.
    Requires object ID from detection result.
  goal:
    object_id:
      type: string
      description: ID of object to grasp (from DetectObjects result)
      example: "obj_0"
    grasp_pose:
      type: geometry_msgs/Pose
      description: Desired grasp pose (relative to object frame)
      optional: true
    grasp_force_n:
      type: float32
      description: Desired grasp force in Newtons
      example: 5.0
      constraints: |
        - Range: 0.0-10.0 N
        - Fragile objects: 2.0-5.0 N
        - Sturdy objects: 5.0-10.0 N
  result:
    success:
      type: bool
      description: Whether grasp succeeded
    error_message:
      type: string
      description: Human-readable error if failed
      example: "Object slipped from gripper"
    grasp_quality:
      type: float32
      description: Estimated grasp quality (0.0-1.0)
      example: 0.88
  feedback:
    current_stage:
      type: string
      description: Current manipulation stage
      enum: [approaching, pre_grasp, grasping, lifting, holding]
    grasp_quality:
      type: float32
      description: Real-time grasp quality estimate

  action_server: /grasp_object
  package: manipulation_msgs (custom)
  typical_duration: 5-15 seconds

#=====================================================
# Feedback Messages (Robot → LLM for Replanning)
#=====================================================

vla_msgs/Feedback:
  description: |
    Generic feedback message for robot status, sensor data, and action results.
    Used by LLM to monitor execution and trigger replanning if needed.
  fields:
    header:
      type: std_msgs/Header
      description: Timestamp when feedback was generated
    feedback_type:
      type: string
      description: Category of feedback
      enum: [SENSOR, ACTION_RESULT, SYSTEM_STATUS, ERROR]
    source:
      type: string
      description: Originating node or sensor
      example: "isaac_ros_yolo", "nav2_controller", "gripper_sensors"
    data_json:
      type: string
      description: Feedback-specific data (JSON encoded for flexibility)
      example: '{"objects_detected": [{"class": "cup", "confidence": 0.92}]}'
    success:
      type: bool
      description: Whether operation succeeded (for action results)
      example: true
    error_message:
      type: string
      description: Human-readable error description (if failed)
      example: "Navigation goal unreachable: obstacle blocking path"
      optional: true

  topic_name: /vla/feedback
  publishers: [nav2_controller, vision_detector, gripper_controller, system_monitor]
  subscribers: [llm_planner_node, telemetry_logger]
  qos:
    reliability: RELIABLE
    durability: VOLATILE  # Only current feedback matters
    history_depth: 20

#=====================================================
# System Status Messages
#=====================================================

vla_msgs/SystemStatus:
  description: |
    Overall VLA system health and readiness.
    Published periodically for monitoring.
  fields:
    header:
      type: std_msgs/Header
    whisper_status:
      type: string
      description: Whisper node status
      enum: [READY, PROCESSING, ERROR, OFFLINE]
    llm_status:
      type: string
      description: LLM planner status
      enum: [READY, PLANNING, ERROR, OFFLINE]
    action_executor_status:
      type: string
      description: Action orchestrator status
      enum: [IDLE, EXECUTING, ERROR, OFFLINE]
    current_action:
      type: string
      description: Currently executing action (if any)
      example: "navigate_to_pose"
      optional: true
    pipeline_latency_ms:
      type: int32
      description: End-to-end latency (voice → action start)
      example: 3500

  topic_name: /vla/system_status
  publishers: [vla_system_monitor]
  subscribers: [dashboard, telemetry]
  qos:
    reliability: BEST_EFFORT
    durability: VOLATILE
    history_depth: 1
  publish_rate: 1 Hz  # Once per second

#=====================================================
# Message Validation Rules
#=====================================================

validation_rules:
  audio_input:
    - Sample rate must be 8kHz-48kHz (16kHz recommended)
    - Duration should be 1-30 seconds for optimal Whisper performance
    - Audio data cannot be empty

  transcription:
    - Confidence must be 0.0-1.0
    - Reject if confidence < 0.7 (too uncertain)
    - Text should be 1-500 characters (filter noise)

  cognitive_plan:
    - All action types must be in allowed whitelist
    - Dependency graph must be acyclic (DAG)
    - Step IDs must be sequential starting from 1
    - params_json must be valid JSON

  robot_actions:
    - Navigation poses must be within workspace bounds
    - Grasp forces must be 0.0-10.0 N
    - Detection confidence thresholds must be 0.5-0.99

  feedback:
    - Timestamps must be recent (< 5 seconds old for critical feedback)
    - data_json must be valid JSON
    - error_message required if success=false

#=====================================================
# Topic Naming Conventions
#=====================================================

topic_conventions:
  namespacing: /vla/{component}/{message_type}
  examples:
    audio_input: /audio/input
    transcription: /voice_command/transcription
    cognitive_plan: /cognitive_plan/output
    feedback: /vla/feedback
    system_status: /vla/system_status

  notes: |
    - Use descriptive topic names (not /topic1, /data)
    - Group related topics under common namespace (/vla/, /audio/, etc.)
    - Follow ROS 2 conventions (lowercase, underscores)

#=====================================================
# QoS Profiles Summary
#=====================================================

qos_guidelines:
  real_time_streams:
    reliability: BEST_EFFORT
    durability: VOLATILE
    use_cases: [audio_input, camera_images, odometry]
    rationale: Latency-critical, dropped messages acceptable

  command_messages:
    reliability: RELIABLE
    durability: TRANSIENT_LOCAL
    use_cases: [transcription, cognitive_plan, action_goals]
    rationale: Must not lose commands, late subscribers need last message

  status_updates:
    reliability: BEST_EFFORT
    durability: VOLATILE
    use_cases: [system_status, periodic_feedback]
    rationale: Frequent updates, only current value matters

#=====================================================
# Notes
#=====================================================

notes: |
  - All message types are conceptual examples for educational purposes
  - Custom messages (whisper_msgs, cognitive_plan_msgs, vla_msgs) would require
    ROS 2 package creation with .msg/.action definitions
  - Standard ROS 2 messages (std_msgs, geometry_msgs, nav2_msgs) are from official packages
  - Message schemas designed for clarity and educational value, not production optimization
  - Students learn data flow and message structure without implementing full ROS 2 packages
