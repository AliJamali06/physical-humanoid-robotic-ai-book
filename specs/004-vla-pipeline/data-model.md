# Data Model: Module 4 – Vision-Language-Action (VLA)

**Feature**: `004-vla-pipeline`
**Date**: 2025-12-09
**Purpose**: Document the 5 conceptual entities in the VLA pipeline and their relationships

## Overview

This data model defines the key conceptual entities in a Vision-Language-Action (VLA) system for humanoid robots. These entities represent the data flow from user voice input through speech recognition, cognitive planning, action execution, and feedback loops.

---

## Entity 1: Voice Command

### Definition
Natural language instruction spoken by a human user, captured as an audio waveform by a microphone.

### Attributes

| Attribute | Type | Description | Example Value |
|-----------|------|-------------|---------------|
| `audio_data` | bytes | Raw audio waveform (16-bit PCM) | Binary audio data |
| `sample_rate` | integer | Audio sampling frequency in Hz | `16000` (16kHz) |
| `duration` | float | Length of audio clip in seconds | `3.5` |
| `timestamp` | datetime | When the command was captured | `2025-12-09T14:23:10Z` |
| `language` | string | Detected or specified language code | `"en-US"` (English, US) |
| `channel_count` | integer | Mono or stereo | `1` (mono) |

### Conceptual Representation

```
Voice Command
├── audio_data: [binary waveform]
├── sample_rate: 16000 Hz
├── duration: 3.5 seconds
├── timestamp: 2025-12-09T14:23:10Z
├── language: "en-US"
└── channel_count: 1 (mono)
```

### ROS 2 Message Type

```yaml
# audio_msgs/Audio (ROS 2 Humble)
std_msgs/Header header
  uint32 seq
  time stamp
  string frame_id
audio_common_msgs/AudioInfo info
  uint32 channels          # 1 for mono, 2 for stereo
  uint32 sample_rate       # 16000 Hz typical
  string sample_format     # "S16LE" (16-bit PCM Little Endian)
  uint32 bitrate
  string coding_format     # "wave", "mp3", "ogg"
uint8[] data               # Raw audio bytes
```

### Example Voice Commands

- "Navigate to the kitchen"
- "Bring me a drink from the table"
- "Pick up the red block"
- "Stop moving"
- "Show me what you see"

### Data Flow

```
Microphone → Audio Capture Node → /audio/input topic → Whisper Node
```

### Constraints and Validation

- **Duration**: Optimal 1-30 seconds (Whisper trained on 30-second chunks)
- **Sample Rate**: 16kHz recommended for Whisper (balances quality and computational efficiency)
- **Format**: Mono preferred (stereo can be downmixed)
- **Quality**: Background noise should be <40dB for reliable recognition

---

## Entity 2: Transcription

### Definition
Text representation of a Voice Command, generated by the Whisper speech recognition model. This is the output of the speech-to-text stage.

### Attributes

| Attribute | Type | Description | Example Value |
|-----------|------|-------------|---------------|
| `text` | string | Transcribed text from audio | `"Navigate to the kitchen"` |
| `confidence` | float | Whisper's confidence score (0.0-1.0) | `0.92` |
| `language` | string | Detected language code | `"en"` |
| `timestamp` | datetime | When transcription was completed | `2025-12-09T14:23:12Z` |
| `model_variant` | string | Whisper model used | `"base"` |
| `processing_time_ms` | integer | Inference latency in milliseconds | `350` |

### Conceptual Representation

```
Transcription
├── text: "Navigate to the kitchen"
├── confidence: 0.92
├── language: "en"
├── timestamp: 2025-12-09T14:23:12Z
├── model_variant: "base"
└── processing_time_ms: 350
```

### ROS 2 Message Type

```yaml
# std_msgs/String (simplified)
string data  # Transcribed text

# Or custom message for richer metadata:
# whisper_msgs/Transcription
std_msgs/Header header
string text                  # Transcribed text
float32 confidence           # 0.0-1.0
string language              # Language code
string model_variant         # "tiny", "base", "small", "medium", "large"
int32 processing_time_ms     # Inference latency
```

### Example Transcriptions

| Voice Command | Transcription | Confidence |
|---------------|---------------|------------|
| "Navigate to the kitchen" | `"Navigate to the kitchen"` | 0.95 |
| "Bring me a drink" | `"Bring me a drink"` | 0.88 |
| "Go to the... um... living room" | `"Go to the living room"` | 0.76 (hesitation handled) |
| "Pick up the red block" (background noise) | `"Pick up the red block"` | 0.82 (robust to noise) |

### Data Flow

```
Whisper Node → /voice_command/transcription topic → LLM Planner Node
```

### Constraints and Validation

- **Confidence Threshold**: Reject transcriptions <0.7 confidence (too uncertain)
- **Length**: Typical 5-50 words (very short or very long may indicate errors)
- **Sanity Check**: Filter profanity, non-language outputs (rare Whisper failure mode)
- **Language Match**: Verify transcription language matches expected robot language setting

---

## Entity 3: Cognitive Plan

### Definition
Structured sequence of sub-tasks generated by an LLM (Large Language Model) that decomposes a high-level natural language command into executable robot behaviors.

### Attributes

| Attribute | Type | Description | Example Value |
|-----------|------|-------------|---------------|
| `task_description` | string | Original command | `"Bring me a drink from the kitchen"` |
| `steps` | array of objects | Ordered sequence of sub-tasks | `[{navigate}, {detect}, {grasp}, ...]` |
| `timestamp` | datetime | When plan was generated | `2025-12-09T14:23:15Z` |
| `llm_model` | string | LLM used for planning | `"gpt-4"` or `"llama-13b"` |
| `processing_time_ms` | integer | LLM inference latency | `2500` |
| `confidence` | float | LLM's self-assessed confidence (if available) | `0.85` |

### Step Structure

Each step in `steps` array contains:

| Attribute | Type | Description | Example Value |
|-----------|------|-------------|---------------|
| `step_id` | integer | Sequential step number | `1` |
| `action` | string | Action type (maps to robot API) | `"navigate_to_pose"` |
| `params` | object | Action-specific parameters | `{"location": "kitchen", "x": 5.0, "y": 3.0}` |
| `dependencies` | array of integers | IDs of steps that must complete first | `[]` (no dependencies for first step) |
| `expected_duration_s` | integer | Estimated execution time | `30` |

### Conceptual Representation

```json
{
  "task_description": "Bring me a drink from the kitchen",
  "steps": [
    {
      "step_id": 1,
      "action": "navigate_to_pose",
      "params": {
        "location": "kitchen",
        "x": 5.0,
        "y": 3.0,
        "theta": 0.0
      },
      "dependencies": [],
      "expected_duration_s": 30
    },
    {
      "step_id": 2,
      "action": "detect_objects",
      "params": {
        "classes": ["cup", "bottle", "glass"],
        "min_confidence": 0.8
      },
      "dependencies": [1],
      "expected_duration_s": 5
    },
    {
      "step_id": 3,
      "action": "grasp_object",
      "params": {
        "object_id": "$detected_object_0"
      },
      "dependencies": [2],
      "expected_duration_s": 10
    },
    {
      "step_id": 4,
      "action": "navigate_to_pose",
      "params": {
        "location": "user",
        "x": 0.0,
        "y": 0.0,
        "theta": 3.14
      },
      "dependencies": [3],
      "expected_duration_s": 30
    },
    {
      "step_id": 5,
      "action": "handover_object",
      "params": {
        "release_condition": "user_confirmation"
      },
      "dependencies": [4],
      "expected_duration_s": 5
    }
  ],
  "timestamp": "2025-12-09T14:23:15Z",
  "llm_model": "gpt-4",
  "processing_time_ms": 2500,
  "confidence": 0.85
}
```

### ROS 2 Message Type

```yaml
# std_msgs/String (JSON serialized)
string data  # JSON representation of plan

# Or custom message:
# cognitive_plan_msgs/CognitivePlan
std_msgs/Header header
string task_description
cognitive_plan_msgs/Step[] steps
  int32 step_id
  string action
  string params_json   # JSON-encoded parameters
  int32[] dependencies
  int32 expected_duration_s
string llm_model
int32 processing_time_ms
float32 confidence
```

### Data Flow

```
LLM Planner Node → /cognitive_plan/output topic → Action Orchestrator Node
```

### Constraints and Validation

- **Action Whitelist**: All `action` values must match robot's allowed API (no hallucinated actions)
- **Parameter Bounds**: Validate position coordinates within workspace, grasp forces within safe limits
- **Dependency Graph**: Must be a directed acyclic graph (DAG) - no circular dependencies
- **Feasibility**: Check object existence (for `detect_objects`), collision-free paths (for `navigate_to_pose`)

---

## Entity 4: Robot Action

### Definition
Grounded executable behavior that maps a Cognitive Plan step to a specific ROS 2 action call with concrete parameters.

### Attributes

| Attribute | Type | Description | Example Value |
|-----------|------|-------------|---------------|
| `action_id` | string | Unique identifier for this action instance | `"nav_001"` |
| `action_type` | string | ROS 2 action name | `"NavigateToPose"` |
| `goal` | object | Action-specific goal parameters | `{"pose": {"x": 5.0, "y": 3.0, "theta": 0.0}}` |
| `status` | enum | Execution state | `"PENDING"`, `"ACTIVE"`, `"SUCCEEDED"`, `"FAILED"`, `"ABORTED"` |
| `start_time` | datetime | When action execution began | `2025-12-09T14:23:20Z` |
| `end_time` | datetime | When action completed | `2025-12-09T14:23:50Z` |
| `result` | object | Action-specific result data | `{"final_pose": {"x": 4.98, "y": 3.02}}` |
| `feedback` | object | Real-time execution feedback | `{"distance_remaining": 0.5}` |

### Conceptual Representation

```json
{
  "action_id": "nav_001",
  "action_type": "NavigateToPose",
  "goal": {
    "pose": {
      "position": {"x": 5.0, "y": 3.0, "z": 0.0},
      "orientation": {"x": 0.0, "y": 0.0, "z": 0.0, "w": 1.0}
    }
  },
  "status": "ACTIVE",
  "start_time": "2025-12-09T14:23:20Z",
  "end_time": null,
  "result": null,
  "feedback": {
    "distance_remaining": 2.5,
    "estimated_time_remaining": 15
  }
}
```

### ROS 2 Action Types (Examples)

#### NavigateToPose (Nav2)

```yaml
# nav2_msgs/action/NavigateToPose.action
# Goal
geometry_msgs/PoseStamped pose
  std_msgs/Header header
  geometry_msgs/Pose pose
    geometry_msgs/Point position
    geometry_msgs/Quaternion orientation
string behavior_tree  # Optional: custom BT XML

# Result
std_msgs/Empty result

# Feedback
geometry_msgs/PoseStamped current_pose
builtin_interfaces/Duration navigation_time
builtin_interfaces/Duration estimated_time_remaining
int16 number_of_recoveries
float32 distance_remaining
```

#### GraspObject (MoveIt2/Custom)

```yaml
# manipulation_msgs/action/GraspObject.action
# Goal
string object_id
geometry_msgs/Pose grasp_pose
float32 grasp_force_n  # Newtons

# Result
bool success
string error_message

# Feedback
string current_stage  # "approaching", "grasping", "lifting"
float32 grasp_quality  # 0.0-1.0
```

### Data Flow

```
Action Orchestrator Node → ROS 2 Action Server (Nav2, MoveIt2) → Robot Hardware
```

### Action Lifecycle

```
PENDING → ACTIVE → SUCCEEDED/FAILED/ABORTED
```

- **PENDING**: Action queued but not started
- **ACTIVE**: Currently executing
- **SUCCEEDED**: Completed successfully
- **FAILED**: Execution failed (e.g., unreachable goal, object not graspable)
- **ABORTED**: Manually stopped or preempted by higher-priority action

### Example Actions

| Cognitive Plan Step | Robot Action Type | Goal Parameters |
|---------------------|-------------------|-----------------|
| `navigate_to_pose("kitchen")` | `NavigateToPose` | `pose: {x: 5.0, y: 3.0, theta: 0.0}` |
| `detect_objects("cup")` | `DetectObjects` | `classes: ["cup"], min_confidence: 0.8` |
| `grasp_object(obj_0)` | `GraspObject` | `object_id: "obj_0", grasp_force: 5.0` |
| `handover_object()` | `ReleaseGripper` | `release_condition: "user_confirmation"` |

### Constraints and Validation

- **Safety Limits**: Navigation speed <0.5 m/s near humans, grasp force <10N for fragile objects
- **Collision Checking**: All navigation goals must be collision-free per costmap
- **Reachability**: Manipulation goals must be within robot's workspace (kinematic limits)
- **Timeout**: Actions must complete within expected duration + margin (abort if stuck)

---

## Entity 5: Feedback

### Definition
Sensor data and execution status returned from the robot to the LLM planner for replanning, confirmation, or error recovery.

### Attributes

| Attribute | Type | Description | Example Value |
|-----------|------|-------------|---------------|
| `feedback_type` | enum | Category of feedback | `"SENSOR"`, `"ACTION_RESULT"`, `"SYSTEM_STATUS"` |
| `timestamp` | datetime | When feedback was generated | `2025-12-09T14:23:50Z` |
| `source` | string | Originating sensor or node | `"camera_front"`, `"nav2_controller"` |
| `data` | object | Feedback-specific data | `{"objects_detected": [...]}` |
| `success` | boolean | Whether operation succeeded (for action results) | `true` |
| `error_message` | string | Human-readable error description (if failed) | `"Goal unreachable: obstacle blocking path"` |

### Feedback Types

#### 1. Sensor Feedback (Perception)

```json
{
  "feedback_type": "SENSOR",
  "timestamp": "2025-12-09T14:23:25Z",
  "source": "isaac_ros_object_detector",
  "data": {
    "objects_detected": [
      {
        "class": "cup",
        "confidence": 0.92,
        "bounding_box": {"x": 320, "y": 240, "width": 80, "height": 120},
        "position_3d": {"x": 1.2, "y": 0.5, "z": 0.8}
      },
      {
        "class": "bottle",
        "confidence": 0.87,
        "bounding_box": {"x": 450, "y": 220, "width": 60, "height": 150},
        "position_3d": {"x": 1.5, "y": 0.3, "z": 0.9}
      }
    ]
  },
  "success": true,
  "error_message": null
}
```

#### 2. Action Result Feedback (Execution Outcome)

```json
{
  "feedback_type": "ACTION_RESULT",
  "timestamp": "2025-12-09T14:23:50Z",
  "source": "nav2_action_server",
  "data": {
    "action_type": "NavigateToPose",
    "goal": {"x": 5.0, "y": 3.0, "theta": 0.0},
    "final_pose": {"x": 4.98, "y": 3.02, "theta": 0.05},
    "distance_error_m": 0.03,
    "navigation_time_s": 30
  },
  "success": true,
  "error_message": null
}
```

#### 3. Failure Feedback (Error Reporting)

```json
{
  "feedback_type": "ACTION_RESULT",
  "timestamp": "2025-12-09T14:24:10Z",
  "source": "grasp_action_server",
  "data": {
    "action_type": "GraspObject",
    "object_id": "obj_0",
    "failure_reason": "object_slipped"
  },
  "success": false,
  "error_message": "Grasp failed: object slipped from gripper (insufficient friction)"
}
```

### ROS 2 Message Type

```yaml
# Custom feedback message:
# vla_msgs/Feedback
std_msgs/Header header
string feedback_type        # "SENSOR", "ACTION_RESULT", "SYSTEM_STATUS"
string source               # Node/sensor name
string data_json            # JSON-encoded feedback data
bool success
string error_message
```

### Data Flow

```
Robot Sensors / Action Servers → Feedback Topic → LLM Planner Node (for replanning)
```

### Use Cases for Feedback

1. **Object Detection Confirmation**: LLM requested "detect cup", feedback confirms cup found at (x, y, z)
2. **Navigation Failure Recovery**: Nav2 reports path blocked, LLM replans with alternative route
3. **Grasp Success Validation**: Gripper sensors confirm object secured, proceed to next step
4. **User Confirmation**: Camera detects user nodding (vision feedback), trigger handover action

### Feedback-Driven Replanning

**Example Scenario**: Navigation obstacle encountered

1. **Initial Plan**: Navigate to kitchen at (5.0, 3.0)
2. **Feedback**: Nav2 reports "obstacle blocking path" at t=15s
3. **LLM Replanning**:
   - Analyze feedback: obstacle at (4.5, 2.8)
   - Generate alternative: navigate via waypoint (6.0, 4.0) then to kitchen
4. **New Plan**: Navigate to (6.0, 4.0), then navigate to (5.0, 3.0)

### Constraints and Validation

- **Freshness**: Sensor feedback <1 second old (stale data can cause failures)
- **Consistency**: Multiple sensors should agree (e.g., camera and LiDAR both detect obstacle)
- **Noise Filtering**: Average multiple readings to reduce sensor noise
- **Safety**: Immediate feedback for collision warnings, e-stop triggers

---

## Entity Relationships

### Data Flow Diagram

```
┌─────────────────┐
│ Voice Command   │  (Audio waveform from user)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Transcription   │  (Text from Whisper)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Cognitive Plan  │  (LLM-generated action sequence)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Robot Action    │  (ROS 2 action execution)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Feedback        │  (Sensor data, action results)
└────────┬────────┘
         │
         └──────────► (Loop back to Cognitive Plan for replanning if needed)
```

### Relationship Table

| Entity 1 | Relationship | Entity 2 | Description |
|----------|--------------|----------|-------------|
| Voice Command | **transcribed_to** | Transcription | Whisper converts audio to text |
| Transcription | **planned_by** | Cognitive Plan | LLM decomposes text into steps |
| Cognitive Plan | **executed_as** | Robot Action | Steps become ROS 2 actions |
| Robot Action | **generates** | Feedback | Actions produce sensor data and results |
| Feedback | **triggers** | Cognitive Plan | Failures or new info trigger replanning |

### Cardinality

- **1 Voice Command → 1 Transcription** (one audio input produces one text output)
- **1 Transcription → 1 Cognitive Plan** (one command produces one plan)
- **1 Cognitive Plan → N Robot Actions** (plan has multiple steps, each maps to an action)
- **1 Robot Action → N Feedback** (action produces continuous feedback during execution)
- **N Feedback → 0..1 Cognitive Plan** (feedback may trigger replanning, or none if successful)

---

## Example End-to-End Scenario

### Scenario: "Bring me a drink from the kitchen"

#### 1. Voice Command
```json
{
  "audio_data": "[binary waveform]",
  "sample_rate": 16000,
  "duration": 2.8,
  "timestamp": "2025-12-09T14:23:10Z",
  "language": "en-US",
  "channel_count": 1
}
```

#### 2. Transcription
```json
{
  "text": "Bring me a drink from the kitchen",
  "confidence": 0.93,
  "language": "en",
  "timestamp": "2025-12-09T14:23:12Z",
  "model_variant": "base",
  "processing_time_ms": 320
}
```

#### 3. Cognitive Plan
```json
{
  "task_description": "Bring me a drink from the kitchen",
  "steps": [
    {"step_id": 1, "action": "navigate_to_pose", "params": {"location": "kitchen", "x": 5.0, "y": 3.0}},
    {"step_id": 2, "action": "detect_objects", "params": {"classes": ["cup", "bottle"], "min_confidence": 0.8}},
    {"step_id": 3, "action": "grasp_object", "params": {"object_id": "$detected_object_0"}},
    {"step_id": 4, "action": "navigate_to_pose", "params": {"location": "user", "x": 0.0, "y": 0.0}},
    {"step_id": 5, "action": "handover_object", "params": {}}
  ],
  "timestamp": "2025-12-09T14:23:15Z",
  "llm_model": "gpt-4",
  "processing_time_ms": 2800
}
```

#### 4. Robot Actions (Executed Sequentially)

**Action 1: Navigate to Kitchen**
```json
{
  "action_id": "nav_001",
  "action_type": "NavigateToPose",
  "goal": {"pose": {"x": 5.0, "y": 3.0, "theta": 0.0}},
  "status": "SUCCEEDED",
  "start_time": "2025-12-09T14:23:20Z",
  "end_time": "2025-12-09T14:23:50Z",
  "result": {"final_pose": {"x": 4.98, "y": 3.02}}
}
```

**Action 2: Detect Objects**
```json
{
  "action_id": "detect_001",
  "action_type": "DetectObjects",
  "goal": {"classes": ["cup", "bottle"], "min_confidence": 0.8},
  "status": "SUCCEEDED",
  "start_time": "2025-12-09T14:23:51Z",
  "end_time": "2025-12-09T14:23:54Z",
  "result": {
    "objects": [
      {"id": "obj_0", "class": "cup", "confidence": 0.92, "position": {"x": 5.1, "y": 3.2}}
    ]
  }
}
```

**Action 3: Grasp Object**
```json
{
  "action_id": "grasp_001",
  "action_type": "GraspObject",
  "goal": {"object_id": "obj_0", "grasp_force": 5.0},
  "status": "SUCCEEDED",
  "start_time": "2025-12-09T14:23:55Z",
  "end_time": "2025-12-09T14:24:05Z",
  "result": {"grasp_quality": 0.88}
}
```

**Action 4: Navigate to User**
```json
{
  "action_id": "nav_002",
  "action_type": "NavigateToPose",
  "goal": {"pose": {"x": 0.0, "y": 0.0, "theta": 3.14}},
  "status": "SUCCEEDED",
  "start_time": "2025-12-09T14:24:06Z",
  "end_time": "2025-12-09T14:24:36Z",
  "result": {"final_pose": {"x": 0.02, "y": 0.01}}
}
```

**Action 5: Handover Object**
```json
{
  "action_id": "handover_001",
  "action_type": "ReleaseGripper",
  "goal": {"release_condition": "user_confirmation"},
  "status": "SUCCEEDED",
  "start_time": "2025-12-09T14:24:37Z",
  "end_time": "2025-12-09T14:24:40Z",
  "result": {"release_confirmed": true}
}
```

#### 5. Feedback (Continuous During Execution)

**Navigation Feedback**
```json
{
  "feedback_type": "ACTION_RESULT",
  "timestamp": "2025-12-09T14:23:35Z",
  "source": "nav2_controller",
  "data": {"distance_remaining": 2.5, "estimated_time_remaining": 15},
  "success": true
}
```

**Detection Feedback**
```json
{
  "feedback_type": "SENSOR",
  "timestamp": "2025-12-09T14:23:54Z",
  "source": "isaac_ros_yolo",
  "data": {
    "objects_detected": [
      {"class": "cup", "confidence": 0.92, "position_3d": {"x": 5.1, "y": 3.2, "z": 0.8}}
    ]
  },
  "success": true
}
```

**Grasp Success Feedback**
```json
{
  "feedback_type": "ACTION_RESULT",
  "timestamp": "2025-12-09T14:24:05Z",
  "source": "gripper_sensors",
  "data": {"grasp_quality": 0.88, "object_secured": true},
  "success": true
}
```

---

## Summary

### Key Entities
1. **Voice Command**: Audio input from user
2. **Transcription**: Text output from Whisper
3. **Cognitive Plan**: LLM-generated action sequence
4. **Robot Action**: Grounded ROS 2 actions
5. **Feedback**: Sensor data and execution results

### Data Flow
```
Voice → Transcription → Cognitive Plan → Robot Actions → Feedback → (Replanning if needed)
```

### ROS 2 Integration
All entities map to ROS 2 messages and actions:
- Voice Command: `audio_msgs/Audio`
- Transcription: `std_msgs/String`
- Cognitive Plan: `std_msgs/String` (JSON)
- Robot Action: ROS 2 Actions (`NavigateToPose`, `GraspObject`, etc.)
- Feedback: Custom feedback messages + action feedback

---

**Data Model Complete**: Ready for use in contracts, code examples, and chapter content
